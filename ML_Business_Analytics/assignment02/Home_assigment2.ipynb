{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgccaP_57U7m"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from matplotlib import cm\n",
        "from scipy import stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, in contrast to the example we solved in the class, we focus on classification problem.\n",
        "To do so, we assign house prices to three classes : cheap, medium and expensive price.\n",
        "On the next step we build the model based on features that predicts class of the house. A robust implementation  must consider feature engineering, data cleaning, and cross-validation."
      ],
      "metadata": {
        "id": "7JW_grG0rsfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling and train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, OneHotEncoder\n",
        "\n",
        "# components for ANN model\n",
        "from keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# evaluation on test data\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
      ],
      "metadata": {
        "id": "DnkN_x_bxT-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "RZ7tlBk5BqSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UL9ItSetIHPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Rxe-YB1hPevW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Colab\\ Notebooks/Housing\\ data"
      ],
      "metadata": {
        "id": "hvjL2MOVG3sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read training data with labels into `train_full_df` and data without labels into `new_input_df`\n",
        "\n"
      ],
      "metadata": {
        "id": "VgMMuEcAmVLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Housing data/train.csv\") # data with labels\n",
        "new_input_df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Housing data/test.csv\") # data without labels"
      ],
      "metadata": {
        "id": "DCj7QBzoJZo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df"
      ],
      "metadata": {
        "id": "QKFDhv27G3vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df.shape"
      ],
      "metadata": {
        "id": "XFaEPy8-AmvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_df.shape"
      ],
      "metadata": {
        "id": "-dqtgFEO5D9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing"
      ],
      "metadata": {
        "id": "nF8pT---Jhkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As first step to develop ML model, we pre-process data similarly how it was performed for regression problem in class assigment."
      ],
      "metadata": {
        "id": "e_a0iwPPmzlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = ['SalePrice']\n",
        "features = train_full_df.drop(['Id'] + target, axis=1).columns\n",
        "#features = train_df.drop(target, axis=1).columns\n",
        "\n",
        "dataset_types = pd.DataFrame(train_full_df[features].dtypes, columns=['datatype'])\n",
        "dataset_types.reset_index(inplace=True)\n",
        "dataset_types"
      ],
      "metadata": {
        "id": "z_b5qRtDDY9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to sort out numerical and categorical features, since they are treated differently by scalling and models"
      ],
      "metadata": {
        "id": "u0_tYo0XnFnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = dataset_types.rename(columns={\"index\" : \"feature\"}).feature[(dataset_types.datatype == 'float64') | (dataset_types.datatype == 'int64')]\n",
        "#num_data = train_full_df[numeric_features]\n",
        "num_data = train_full_df[numeric_features]\n",
        "num_features = num_data.fillna(num_data.mean()).values\n"
      ],
      "metadata": {
        "id": "E2rwC8r5Dmox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply\n",
        "`\n",
        "StandardScaler\n",
        "`\n",
        "to adjust numerical features  by removing the mean and scaling to unit variance.\n"
      ],
      "metadata": {
        "id": "h7WoVdQIoB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "num_features_scaled = scaler.fit_transform(num_features)\n",
        "num_features_scaled"
      ],
      "metadata": {
        "id": "gFPyy-YooChj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply\n",
        "`\n",
        "LabelEncoder\n",
        "`\n",
        "and\n",
        "`\n",
        "OneHotEncoder`\n",
        "\n",
        "\n",
        "to adjust categorical features by encoding target labels with value between 0 and n_classes-1 and as a one-hot numeric array.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "USOROKyToaBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_one_categorical_feature(column):\n",
        "    le = LabelEncoder()\n",
        "    ohe = OneHotEncoder(sparse_output=False)\n",
        "    num_encoded = le.fit_transform(column.fillna('unk'))\n",
        "    oh_encoded = ohe.fit_transform(num_encoded.reshape(-1, 1))\n",
        "    return oh_encoded"
      ],
      "metadata": {
        "id": "GKDYBmuAENbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = dataset_types.rename(columns={\"index\" : \"feature\"}).feature[(dataset_types.datatype == 'object')]\n",
        "cat_data = train_full_df[categorical_features]\n",
        "cat_data_new = new_input_df[categorical_features]\n",
        "\n",
        "cat_data_combined=pd.concat([cat_data,cat_data_new],ignore_index=True)\n",
        "cat_features_combined = np.hstack([encode_one_categorical_feature(cat_data_combined[column]) for column in cat_data.columns])\n"
      ],
      "metadata": {
        "id": "9Xv7h8wtGXKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olsxa9ClbJlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features=cat_features_combined[len(cat_data)]"
      ],
      "metadata": {
        "id": "J4XDxg6VHfFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features_combined[:len(cat_data),:].shape"
      ],
      "metadata": {
        "id": "UqAfUfdGHuqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features_combined[len(cat_data):,:].shape"
      ],
      "metadata": {
        "id": "ukpniWbiIDdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cat_data)"
      ],
      "metadata": {
        "id": "nbWls_arHZXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_data = train_full_df[categorical_features]\n",
        "cat_data_new = new_input_df[categorical_features]\n",
        "cat_data_combined=pd.concat([cat_data,cat_data_new],ignore_index=True)\n",
        "\n",
        "print(cat_data.shape, cat_data_new.shape, cat_data_combined.shape)"
      ],
      "metadata": {
        "id": "htCBQsINGrrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = dataset_types.rename(columns={\"index\" : \"feature\"}).feature[(dataset_types.datatype == 'object')]\n",
        "cat_data = train_full_df[categorical_features]\n",
        "#cat_data = train_df[categorical_features]\n",
        "cat_features = np.hstack([encode_one_categorical_feature(train_full_df[column]) for column in cat_data.columns])\n",
        "cat_features"
      ],
      "metadata": {
        "id": "6ry5fs48D8HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_data.shape"
      ],
      "metadata": {
        "id": "dbB1-VjlOeIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_df[categorical_features].shape"
      ],
      "metadata": {
        "id": "i0rtieVoOnhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features"
      ],
      "metadata": {
        "id": "pm82DDk7CupX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, val in enumerate(categorical_features):\n",
        "  print(val)\n",
        "  print( set(train_full_df[val].unique())- set(new_input_df[val].unique()) )"
      ],
      "metadata": {
        "id": "WFrRK0JKOuGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df[\"Condition2\"].unique()"
      ],
      "metadata": {
        "id": "3O73nCc5OuKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input_df[\"Condition2\"].unique()"
      ],
      "metadata": {
        "id": "8GPxJm95E48F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0XurUxDGJxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There are {} features in this dataset\".format(len(train_full_df.columns)))\n",
        "print(\"{} features are numeric\".format(len(numeric_features)))\n",
        "print(\"{} features are categorical.\".format(len(categorical_features)))\n",
        "print(\"The last two are the target, which is numeric, and the id column.\")"
      ],
      "metadata": {
        "id": "zW_l5CXOCsaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we stack together numerical and categorical features into\n",
        "`\n",
        "X\n",
        "` variable and targets into\n",
        "`\n",
        "y\n",
        "`. Next we split into train and test splits.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RY7DzeiAqU5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.hstack((num_features_scaled, cat_features))\n"
      ],
      "metadata": {
        "id": "9_oNK2BrBsbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to transform numerical SalePrice target to categorical. For this we first calculate quantiles of SalePrice.\n",
        "Next, make categories depending to which quantile SalePrice belongs.\n",
        "We label cheap houses with 0, medium with 1 and expansive  with 2"
      ],
      "metadata": {
        "id": "Mv5CvWEkuyVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qn=np.quantile(train_full_df[target].values,[0,0.25,0.5,0.75,1])"
      ],
      "metadata": {
        "id": "jjl6OaOqvk72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create `Pricelabel` column with the default value 0. This would represent the lowest \"cheap\" houses class with label `0`.\n",
        "\n"
      ],
      "metadata": {
        "id": "-LlMFVQo80pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df[\"Pricelabel\"]=0\n"
      ],
      "metadata": {
        "id": "WVBkUB25wPUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define `Pricelabel`between 1st and 3rd quantile to have a medium price class with label `1`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LuNEeteD9QZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df.loc[(train_full_df[\"SalePrice\"]>qn[1]) & (train_full_df[\"SalePrice\"]<qn[3]),\"Pricelabel\" ]=1"
      ],
      "metadata": {
        "id": "J8BfcQej7gIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define `Pricelabel` larger than 3rd quantile to belong to the expansive price class with label `2`.\n"
      ],
      "metadata": {
        "id": "cmq7ubKG9e93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_df.loc[train_full_df[\"SalePrice\"]>=qn[3],\"Pricelabel\" ]=2\n"
      ],
      "metadata": {
        "id": "8FlE4E1zyqwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_full_df[train_full_df[\"Pricelabel\"]==2])"
      ],
      "metadata": {
        "id": "JEmobF6pQtuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_full_df)"
      ],
      "metadata": {
        "id": "PHLZeT9JTX72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question part 1"
      ],
      "metadata": {
        "id": "4je6WQqr9-9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. To which quantile of SalePrice belong data with Pricelabel = 0?\n",
        "2. How many samples (data points) are there in each class. Is it balanced?\n",
        "\n"
      ],
      "metadata": {
        "id": "mfANlVXc-Dkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_full_df[\"Pricelabel\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=404)"
      ],
      "metadata": {
        "id": "ExewTT9zrYPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ANN model"
      ],
      "metadata": {
        "id": "30_M8ztnKU0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create function to plot validation and training accuracy as function of number of epochs to perform cross-validation."
      ],
      "metadata": {
        "id": "nf-WmQVGrhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['loss'], 'b')\n",
        "    plt.plot(history.history['val_loss'], 'r')\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KDKGRZ_fBsdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create a Keras model. It takes in train and test splits, compiles model, fits model, plots training history, calculates model error on test data and returns trained model and training history"
      ],
      "metadata": {
        "id": "nj5cIGDPr4Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1dK2JOYr3lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keras_model(X_train, X_test, y_train, y_test):\n",
        "    NUM_EPOCHS = 50 # set number of epochs for training\n",
        "    BATCH_SIZE = 128 # for faster calculations and model training we split data into batches. This would specify the batch size\n",
        "\n",
        "    inputs = Input(shape=(303, )) # input layer of NN. Here we sepcify number of inputs\n",
        "    x = Dropout(0.2)(inputs) # to make our NN more robust against overfitting we introduce dropout layer\n",
        "                              # this randomly switches off 0.2 (20%) of the connections between layers\n",
        "\n",
        "    x = Dense(128)(x) # next dense layer has 256 neurons\n",
        "    x = Activation(\"relu\")(x) # we specify activation funcion of the layer as RELU function\n",
        "    x = Dropout(0.2)(x) # we introduce droput function between following layers\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "    x = Dense(3)(x) # the last layer has 3 neuron, since we are interested in the prediction of 3 classess\n",
        "    predictions= Activation(\"softmax\")(x) # the last activation is softmax for multiple classes, that is analogy of sigmoid for logistic regressions\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[predictions]) # this collects the model together with specification of the input and output\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\") # here we compile model and specify loss function as well as optimiser.\n",
        "                                                # For classification problems the common choice would be sparse_categorical_crossentropy.\n",
        "    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_split=0.2, verbose=1)\n",
        "    # Here we fit the model with specified train set, batch size, number of epochs and automatically validation split:\n",
        "    # The fit function splits 0.2 of train set into validation set. Important this set is different from test set, that is used for final error calculation\n",
        "\n",
        "    plot_history(history) # plot training curves\n",
        "\n",
        "    score = model.evaluate(X_test, y_test, verbose=0) # evaluate model on the hold-out test set, that was not used during training and validation\n",
        "    print(\"Test MSE is {:.2e}\".format(score))\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "l6geTKi2BUuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = keras_model(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "9Rux-nzKJaYJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question part 2"
      ],
      "metadata": {
        "id": "uuatIxGwKKMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Summarize Keras implementation differences between regression model and classification.\n",
        "2.   Analyse train and test training curves. At which epoch shall we stop training ?\n",
        "\n"
      ],
      "metadata": {
        "id": "pf9WlHefKTFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model performance analysis"
      ],
      "metadata": {
        "id": "n6T0OBneK0vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section we analyse prediction of the model on the test (hold-out) set."
      ],
      "metadata": {
        "id": "9zVgj4LIKs4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.model.predict(X_test) # prediction of the model on hold-out test data set\n"
      ],
      "metadata": {
        "id": "EFmLiXK-Jafj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result of prediction we get matrix with 3 columns with numbers from 0 to 1. The higher the number, the higher is pseudo-probability to belong to one of 3 classes."
      ],
      "metadata": {
        "id": "u-1KDHP-Tf84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question part 3"
      ],
      "metadata": {
        "id": "BO7X5g3YTNQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Print `\n",
        "predicted\n",
        "` matrix.  How would you interpret 3 column values in\n",
        "` predicted\n",
        "`? Can they be negative? Their sum is close to which number?"
      ],
      "metadata": {
        "id": "k6vfpvTtTRB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to assign the exact class we apply\n",
        "`\n",
        "np.argmax\n",
        "`\n",
        "function, that returns the class with highest probability.\n"
      ],
      "metadata": {
        "id": "U2hE4nR-LBRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = np.argmax(predicted, axis=-1)\n"
      ],
      "metadata": {
        "id": "GujC-VdBCKdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we plot visually confusion matrix. For this we apply\n",
        "```\n",
        "confusion_matrix((y_true, y_predicted)\n",
        "```\n"
      ],
      "metadata": {
        "id": "V1FKPHUFUxgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix=confusion_matrix(y_test, predicted_classes)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "XhNJQj4nCrtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section we write function that"
      ],
      "metadata": {
        "id": "q6Qu958WVG27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_conf_matrix(conf_matrix):\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.matshow(conf_matrix, cmap=cm.jet, alpha=0.3)\n",
        "  for i in range(conf_matrix.shape[0]):\n",
        "      for j in range(conf_matrix.shape[1]):\n",
        "          ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "  plt.xlabel('Predictions', fontsize=18)\n",
        "  plt.ylabel('Actuals', fontsize=18)\n",
        "  plt.title('Confusion Matrix', fontsize=18)\n",
        "  plt.show()\n",
        "\n",
        "plot_conf_matrix(conf_matrix)"
      ],
      "metadata": {
        "id": "EKy2WI0sVHED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your code part 1"
      ],
      "metadata": {
        "id": "UHjLs4YkV8-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse confusion matrix and fill in numbers for the print out statements below"
      ],
      "metadata": {
        "id": "JLGzSR6nWAv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of correct predicted cheap houses: ',     )\n",
        "print('Number of correct predicted medium houses: ',     )\n",
        "print('Number of correct predicted expensive houses: ',     )\n",
        "\n",
        "print('Number of predicted false negative cheap houses: ',     )\n",
        "print('Number of predicted false positives cheap houses: ',     )\n",
        "\n",
        "print('Number of predicted false negative medium houses: ',     )\n",
        "print('Number of predicted false positives medium houses: ',     )\n",
        "\n",
        "print('Number of predicted false negative expensive houses: ',     )\n",
        "print('Number of predicted false positives expensive houses: ',     )\n",
        "\n",
        "print('Class with the best accuracy: ',     )\n",
        "print('Class with the worst accuracy: ',     )"
      ],
      "metadata": {
        "id": "53IwnypkWBBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your code part 2"
      ],
      "metadata": {
        "id": "ZWJUzBywZi1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to how good is our classification we can use following metric:\n",
        "\n",
        "\n",
        "*   precision - the number of true positives\n",
        "divided by the total number of elements labeled as belonging to the positive class: `precision=true_positive/(true_positive+false_positive)`\n",
        "*   recall - the number of true positives\n",
        "divided by the total number of elements that actually belong to the positive class: `recall=true_positive/(true_positive+false_negative)`\n",
        "*    F1 score - is given by the ratio of numerator:\n",
        "`2*precision*recall` to denominator: `(precision + recall)`. This is a better choice for unbalanced classes.\n",
        "\n",
        "Implement below this metrics for each class based on the confusion matrix above. Calculate average between 3 classes of F1 score."
      ],
      "metadata": {
        "id": "xQcBDx7tXk4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision_class_1=\n",
        "precision_class_2=\n",
        "precision_class_3=\n",
        "\n",
        "recall_class_1=\n",
        "recall_class_2=\n",
        "recall_class_3=\n",
        "\n",
        "F1_class_1=\n",
        "F1_class_2=\n",
        "F1_class_3=\n",
        "\n",
        "F1_average="
      ],
      "metadata": {
        "id": "ZjipwJdkWLFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your code part 3"
      ],
      "metadata": {
        "id": "yyk2OgldabRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  Implement below following architectures of ANNs for classification:\n",
        "\n",
        "\n",
        "*   3 hidden layers with 256 neurons each, droupot with 40% probability and relu activation function\n",
        "*    5 hidden layers with 128 neurons each, droupot with 20% probability and relu activation function\n",
        "*    2 hidden layers with 512 neurons each, droupot with 20% probability and relu activation function\n",
        "\n",
        "2.   Analyse performance of the models above based on the test data set:\n",
        "\n",
        "*   Plot training curves for each model\n",
        "*   Calculate and draw confusion matrix\n",
        "*   Calculate average F1 score for each of the ANNs above\n",
        "*   Compare training curves, confusion matrix and F1 score between the models\n",
        "\n",
        "3. Chose the best best model based on F1 score. On which epoch it should be stopped ?\n",
        "\n",
        "4. Apply the best performing model to for the prediction on the `new_input_df` dataframe, that we read from `test.csv` at the beginning of the assigment. Important: apply same pre-processing of the input features, such that it is compatiable with developed model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zgc62C3mHPT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question part 3"
      ],
      "metadata": {
        "id": "jvLVA7NAfujK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize below possible application of classification model you have just build:\n",
        "\n",
        "\n",
        "1.   In which case it could be applied? When one should prefer classification model over regression model for houses price prediction?\n",
        "2.   How one can improve this model? What additional data one can collect and include ?\n",
        "3.   How you can imagine to deploy or use such a model?\n",
        "4.   Suggest a way to measure business impact of using the model for houses price prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzGkTYeVfzbY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gwn-fnytfqkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5P0SefQDgB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}