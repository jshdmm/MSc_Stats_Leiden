library(tidyverse)
library(gtsummary)
library(dplyr)
library(ggplot2)
gitdir = "/Users/joshd/Documents/GitHub/SHQ_DVS/"
# load data
df_combined = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/APOE_SHQ_combined_wayfinding.csv', sep = '\t')
View(df_combined)
df_DVS_2 = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/raw/DVS_2.csv', sep = ';')
df_user_apoe = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_user_apoe.csv', sep = '\t')
df_flare_acc = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_flare_acc.csv', sep = '\t')
df_wayfinding = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_wayfinding.csv', sep = '\t')
df_draw = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/raw/DVS_data_n73.csv', sep = ';')
##
d = subset(df_draw, APOE == 'e3/e3', c('VLM_T'))
#subsetting APOE
df_APOE = subset(df_DVS_2, APOE %in% c("e3/e3", "e3/e4"))
library(tidyverse)
library(gtsummary)
library(dplyr)
library(ggplot2)
gitdir = "/Users/joshd/Documents/GitHub/SHQ_DVS/"
# load data
df_combined = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/APOE_SHQ_combined_wayfinding.csv', sep = '\t')
df_DVS_2 = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/raw/DVS_2.csv', sep = ';')
df_user_apoe = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_user_apoe.csv', sep = '\t')
df_flare_acc = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_flare_acc.csv', sep = '\t')
df_wayfinding = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/df_wayfinding.csv', sep = '\t')
df_draw = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/raw/DVS_data_n73.csv', sep = ';')
##
d = subset(df_draw, APOE == 'e3/e3', c('VLM_T'))
#subsetting APOE
df_APOE = subset(df_DVS_2, APOE %in% c("e3/e3", "e3/e4"))
View(df_combined)
# Fit ANCOVA model
model_ancova <- lm(distance_norm ~ APOE + Age + Sex, data = df_combined)
# Fit ANCOVA model
model_ancova <- lm(distance_norm ~ APOE + age + Sex, data = df_combined)
# Fit ANCOVA model
model_ancova <- lm(distance_norm ~ APOE + age + sex, data = df_combined)
# Fit ANCOVA model
model_ancova <- lm(distance_norm ~ APOE + age + gender, data = df_combined)
# Summary of ANCOVA
summary(model_ancova)
#get all wayfinding levels
df_APOE_wayfinding = subset(df_APOE, type %in% c("wayfinding"))
View(df_APOE)
concatenated_df <- df_draw = read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/SHQ_APOE_full_df.csv', sep = ';')
concatenated_df <- read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/SHQ_APOE_full_df.csv', sep = ';')
View(concatenated_df)
concatenated_df <- read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/SHQ_APOE_full_df.csv', sep = '\t')
View(concatenated_df)
concatenated_df <- read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/SHQ_APOE_full_df.csv', sep = ',')
View(concatenated_df)
#subsetting APOE
df_APOE = subset(concatenated_df, APOE %in% c("e3/e3", "e3/e4"))
concatenated_df <- read.csv('/Users/joshd/Documents/GitHub/SHQ_DVS/data/processed/SHQ_APOE_full_df.csv', sep = ',')
#subsetting APOE
df_APOE = subset(concatenated_df, APOE %in% c("e3/e3", "e3/e4"))
#get all wayfinding levels
df_APOE_wayfinding = subset(df_APOE, type %in% c("wayfinding"))
View(df_APOE_wayfinding)
# Fit ANCOVA model
model_ancova <- lm(distance_norm ~ APOE + age + gender, data = df_APOE_wayfinding)
# Summary of ANCOVA
summary(model_ancova)
knitr::opts_chunk$set(echo = TRUE)
train <- readRDS("masq_train.Rda")
test <- readRDS("masq_test.Rda")
View(test)
View(train)
colnames(train)
MASQ = train[, 12:100]
hist(cor(MASQ))
library(glmnet)
x = model.matrix(D_DEPDYS ~ ., data = train)
x_test = model.matrix(D_DEPDYS ~ ., data = test)
y = train$D_DEPDYS
# setup
lasso = cv.glmnet(x, y, alpha = 1)
ridge = cv.glmnet(x, y, alpha = 0)
elastic = cv.glmnet(x, y, alpha = 0.5)
# predict models
predict_L = predict(lasso, s = "lambda.min", newx = x_test)
predict_R = predict(ridge, s = "lambda.min", newx = x_test)
predict_E = predict(elastic, s = "lambda.min", newx = x_test)
# get mean squared errors
MSE_L = mean((predict_L - test$D_DEPDYS)^2)
MSE_R = mean((predict_R - test$D_DEPDYS)^2)
MSE_E = mean((predict_E - test$D_DEPDYS)^2)
# benchmark
MSE_MAX = var(test$D_DEPDYS)
# cross-validated R2
CVR2_L = 1 - MSE_L/MSE_MAX
CVR2_R = 1 - MSE_R/MSE_MAX
CVR2_E = 1 - MSE_E/MSE_MAX
library(glmnet)
x = model.matrix(D_DEPDYS ~ ., data = train)
x_test = model.matrix(D_DEPDYS ~ ., data = test)
y = train$D_DEPDYS
# setup
lasso = cv.glmnet(x, y, alpha = 1)
ridge = cv.glmnet(x, y, alpha = 0)
elastic = cv.glmnet(x, y, alpha = 0.5)
# predict models
predict_L = predict(lasso, s = "lambda.min", newx = x_test)
predict_R = predict(ridge, s = "lambda.min", newx = x_test)
predict_E = predict(elastic, s = "lambda.min", newx = x_test)
# get mean squared errors
MSE_L = mean((predict_L - test$D_DEPDYS)^2)
MSE_R = mean((predict_R - test$D_DEPDYS)^2)
MSE_E = mean((predict_E - test$D_DEPDYS)^2)
# benchmark
MSE_MAX = var(test$D_DEPDYS)
# cross-validated R2
CVR2_L = 1 - MSE_L/MSE_MAX; CVR2_L
CVR2_R = 1 - MSE_R/MSE_MAX; CVR2_R
CVR2_E = 1 - MSE_E/MSE_MAX; CVR2_E
library(glmnet)
x = model.matrix(D_DEPDYS ~ ., data = train)
x_test = model.matrix(D_DEPDYS ~ ., data = test)
y = train$D_DEPDYS
# setup
lasso = cv.glmnet(x, y, alpha = 1); lasso
ridge = cv.glmnet(x, y, alpha = 0); ridge
elastic = cv.glmnet(x, y, alpha = 0.5); elastic
# predict models
predict_L = predict(lasso, s = "lambda.min", newx = x_test)
predict_R = predict(ridge, s = "lambda.min", newx = x_test)
predict_E = predict(elastic, s = "lambda.min", newx = x_test)
# get mean squared errors
MSE_L = mean((predict_L - test$D_DEPDYS)^2)
MSE_R = mean((predict_R - test$D_DEPDYS)^2)
MSE_E = mean((predict_E - test$D_DEPDYS)^2)
# benchmark
MSE_MAX = var(test$D_DEPDYS)
# cross-validated R2
CVR2_L = 1 - MSE_L/MSE_MAX; CVR2_L
CVR2_R = 1 - MSE_R/MSE_MAX; CVR2_R
CVR2_E = 1 - MSE_E/MSE_MAX; CVR2_E
preds_L_1se = predict(lasso, newx = x[x_test, ], type = "response")
preds_L_min = predict(lasso, newx = x[x_test, ], type = "response",
s = "lambda.min")
tab_L_1se = prop.table(table(preds_L_1se > .5, y[x_test]))
tab_L_min = prop.table(table(preds_L_min > .5, y[x_test]))
tab_L_1se; tab_L_min
sum(diag(tab_L_1se)); sum(diag(tab_L_min))
#packages
library(ISLR2)
#packages
library("ISLR2")
#packages
install.packages("ISLR")
library("ISLR2")
#packages
install.packages("ISLR2")
library("ISLR2")
install.packages("nFactors")
library("nFactors")
install.packages("GPArotation")
library("GPArotation")
install.packages("psych")
install.packages("psych")
library("psych")
install.packages("EFA.dimensions")
library("EFA.dimensions")
# checkup
str(College)
# checkup
data("ISLR2")
library("ISLR2")
str(College)
dim(College)
is.na(College)
# clear out variables
data_College = College[,-c(1,2)]
View(data_College)
# centered (default) and normalised variables
pca_result_sc = prcomp(data_College, scale=TRUE)
# not normalised/scaled
pca_result_unsc = prcomp(data_College, scale=FALSE)
# Explained variance per component - normalised
summary(pca_result_sc) # gives output for the variance explained
# Explained variance per component - not normalised
summary(pca_result_unsc)
# Explained variance per component plotted
par(mfrow=c(1,2))
plot(pca_result_sc)
plot(pca_result_unsc)
ev <- eigen(cor(data_College))
ev$values
cor(data_College)
# We first save PVE and cumulative PVE from the summary
prop_var_expl_sc = summary(pca_result_sc)$importance[2,]
cumul_prop_var_expl_sc = summary(pca_result_sc)$importance[3,]
prop_var_expl_unsc = summary(pca_result_unsc)$importance[2,]
cumul_prop_var_expl_unsc = summary(pca_result_unsc)$importance[3,]
# scree plot
par(mfrow=c(2,2))
plot(prop_var_expl_sc, type="b", xlab="Component number (sc.)",
ylab="Proportion variance explained", ylim=c(0,1) )
plot(cumul_prop_var_expl_sc, type="b", xlab="Number of components (sc.)",
ylab="Cumulative prop. var. expl.", ylim=c(0,1) )
plot(prop_var_expl_unsc, type="b", xlab="Component number (unsc.)",
ylab="Proportion variance explained", ylim=c(0,1) )
plot(cumul_prop_var_expl_unsc, type="b", xlab="Number of components (unsc.)",
ylab="Cumulative prop. var. expl.", ylim=c(0,1) )
cumul_prop_var_expl <- cumul_prop_var_expl_sc
nElem=length(cumul_prop_var_expl_sc)
ratios=(cumul_prop_var_expl[2:(nElem-1)]-cumul_prop_var_expl[1:(nElem-2)])/
(cumul_prop_var_expl[3:nElem]-cumul_prop_var_expl[2:(nElem-1)])
ratios
nComponents_selected = which.max( ratios ) + 1
nComponents_selected
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/06')
CESD <- read.csv('data_alexithymia2.csv')
View(CESD)
CESD <- read.csv('data_alexithymia2.csv', sep = ',')
CESD <- read.csv('data_alexithymia2.csv', sep = ',')
CESD <- read.csv('data_alexithymia2.csv', sep = ';')
View(CESD)
#checkup
(str(CESD))
names(CESD)
apply(CESD, 2, mean)
dim(CESD)
#check for missing values
sum(is.na(CESD))
apply(CESD, 2, mean)
apply(CESD$Sex, 2, mean)
apply(CESD$Sex, mean)
apply(CESD$, 1, mean)
apply(CESD, 1, mean)
apply(CESD, 2, mean)
#checkup
str(CESD)
colMeans(CESD$Age)
?colMeans
?colMeans()
CESD$Age
rowMeans(CESD$Age)
rowMeans(CESD[1:2])
View(CESD)
CESD[3:22]
CESD[3:24]
# convert from int to numeric data type
CESD[3:24] <- lapply(CESD[3:24], as.numeric())
# convert from int to numeric data type
CESD[3:24] <- lapply(CESD[3:24], as.numeric
# convert from int to numeric data type
CESD[3:24] <- lapply(CESD[3:24], as.numeric)
#checkup
str(CESD)
# convert from int to numeric data type
CESD[, 3:24] <- lapply(CESD[,3:24], as.numeric)
#checkup
str(CESD)
apply(CESD, 2, mean)
View(CESD)
rowMeans(CESD[1:2])
mean(CESD$X.confused)
apply(CESD, 2, var)
apply(CESD, 2, mean)
apply(CESD, 2, var)
apply(CESD, 2, mean)
apply(CESD, 2, mean, na.rm = TRUE)
apply(CESD, 2, mean, na.rm = TRUE)
apply(CESD, 2, var)
# run PCA with normalized variables
pca_result_sc = prcomp(CESD, scale=TRUE)
View(CESD)
#exclude Sex and ID since they are not continous variables
CESD = CESD[,-c(1,2)]
View(CESD)
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(CESD, scale=TRUE)
View(pca_result_sc)
# means before centering
pr.out$center
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(CESD, scale=TRUE)
# means before centering
pca_result_sc$center
# standard deviations before centering
pca_result_sc$scale
View(CESD)
# Prinicipal component loadings
pca_result_sc$rotation
# dimensionality
dim(pca_result_sc$x)
biplot(pca_result_sc, scale = 0, cex = 0.5)
# standard deviations of each principal component
pca_result_sc$sdev
#variance
pc.var <- pca_result_sc$sdev^2
pc.var
# proportion of variance explained by each principal component
pve <- pc.var / sum(pr.var)
pve
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
summary(pca_result_sc)
# Decicde on how many components to keep using Kaiser's rule
ev <- eigen(cor(CESD))
ev$values
pca_result_sc$rotation = -pca_result_sc$rotation
pca_result_sc$x = -pca_result_sc$x
biplot(pr.out, scale = 0, cex = 0.5)
pca_result_sc$rotation = -pca_result_sc$rotation
pca_result_sc$x = -pca_result_sc$x
biplot(pca_result_sc, scale = 0, cex = 0.5)
summary(pca_result_sc)
summary(pca_result_sc)
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
source("~/.active-rstudio-document", echo=TRUE)
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/06')
CESD <- read.csv('data_alexithymia2.csv', sep = ';')
View(CESD)
#checkup
str(CESD)
names(CESD)
dim(CESD)
#check for missing values
sum(is.na(CESD))
# convert from int to numeric data type
CESD[, 3:24] <- lapply(CESD[,3:24], as.numeric)
apply(CESD, 2, mean, na.rm = TRUE)
apply(CESD, 2, var)
#exclude Sex and ID since they are not continous variables, exclude CESD
TAS_20 = CESD[,-c(1,2,23)]
View(TAS_20)
#exclude ID, sex, age, and CESD
TAS_20 = CESD[,-c(1,2,3, 23)]
View(TAS_20)
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(TAS_20, scale=TRUE)
summary(pca_result_sc)
# means before centering
pca_result_sc$center
# standard deviations before centering
pca_result_sc$scale
View(TAS_20)
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/06')
CESD <- read.csv('data_alexithymia2.csv', sep = ';')
#checkup
str(CESD)
names(CESD)
dim(CESD)
#check for missing values
sum(is.na(CESD))
# convert from int to numeric data type
CESD[, 3:24] <- lapply(CESD[,3:24], as.numeric)
apply(CESD, 2, mean, na.rm = TRUE)
apply(CESD, 2, var)
View(CESD)
#exclude ID, sex, age, and CESD
TAS_20 = CESD[,-c(1,2,3, 24)]
View(TAS_20)
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(TAS_20, scale=TRUE)
summary(pca_result_sc)
# means before centering
pca_result_sc$center
# standard deviations before centering
pca_result_sc$scale
# Prinicipal component loadings
pca_result_sc$rotation
# dimensionality
dim(pca_result_sc$x)
# plot first 2 PC's
pca_result_sc$rotation = -pca_result_sc$rotation
pca_result_sc$x = -pca_result_sc$x
biplot(pca_result_sc, scale = 0, cex = 0.5)
# standard deviations of each principal component
pca_result_sc$sdev
#variance
pc.var <- pca_result_sc$sdev^2
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Decicde on how many components to keep using Kaiser's rule
ev <- eigen(cor(CESD))
ev$values # we keep 7 PC's
apply(TAS_20, 2, mean, na.rm = TRUE)
apply(TAS_20, 2, var)
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(TAS_20, scale=TRUE)
summary(pca_result_sc)
# means before centering
pca_result_sc$center
# Prinicipal component loadings
pca_result_sc$rotation
#checkup
str(CESD)
names(CESD)
dim(CESD)
#check for missing values
sum(is.na(CESD))
# convert from int to numeric data type
CESD[, 3:24] <- lapply(CESD[,3:24], as.numeric)
#exclude ID, sex, age, and CESD
TAS_20 = CESD[,-c(1,2,3, 24)]
apply(TAS_20, 2, mean, na.rm = TRUE)
apply(TAS_20, 2, var)
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(TAS_20, scale=TRUE)
summary(pca_result_sc)
# means before centering
pca_result_sc$center
# standard deviations before centering
pca_result_sc$scale
# Prinicipal component loadings
pca_result_sc$rotation
# dimensionality
dim(pca_result_sc$x)
# plot first 2 PC's
pca_result_sc$rotation = -pca_result_sc$rotation
pca_result_sc$x = -pca_result_sc$x
biplot(pca_result_sc, scale = 0, cex = 0.5)
# standard deviations of each principal component
pca_result_sc$sdev
#variance
pc.var <- pca_result_sc$sdev^2
# Prinicipal component loadings
pca_result_sc$rotation
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
sum(PVE[1:7])
sum(pve[1:7])
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# plot first 2 PC's
pca_result_sc$rotation = -pca_result_sc$rotation
pca_result_sc$x = -pca_result_sc$x
biplot(pca_result_sc, scale = 0, cex = 0.5)
# Prinicipal component loadings
pca_result_sc$rotation
