title("Beta(5,4) posterior density", cex=20)
dev.off()
density2
# Generate sequences of x values
x <- seq(0, 1, by = 0.01)
# Load necessary libraries
library(ggplot2)
# Define values of alpha and beta for each Beta distribution
alphas <- c(1, 4, 0.9, 3, 1)
betas <- c(1, 4, 0.9, 1, 3)
# Generate sequences of x values
x <- seq(0, 1, by = 0.01)
# Create a dataframe to store results
df <- data.frame(x = rep(x, times = length(alphas)),
alpha = rep(alphas, each = length(x)),
beta = rep(betas, each = length(x)))
# Calculate densities for each combination of alpha and beta
df$density <- dbeta(df$x, df$alpha, df$beta)
# Plot the densities
ggplot(df, aes(x = x, y = density, color = factor(alpha))) +
geom_line() +
facet_wrap(~beta, ncol = 3) +
labs(x = "x", y = "Density", title = "Beta Distributions with Different Parameters") +
scale_color_discrete(name = "Alpha") +
theme_minimal()
df
View(df)
# Function to compute mean using formulas
compute_mean_formula <- function(alpha, beta) {
return(alpha / (alpha + beta))
}
# Function to simulate samples and compute mean
compute_mean_simulation <- function(alpha, beta, n_samples = 10000) {
samples <- rbeta(n_samples, alpha, beta)
return(mean(samples))
}
means_formula <- sapply(params, function(param) compute_mean_formula(param[1], param[2]))
params <- list(
Beta_1_1 = c(1, 1),
Beta_4_4 = c(4, 4),
Beta_0.9_0.9 = c(0.9, 0.9),
Beta_3_1 = c(3, 1),
Beta_1_3 = c(1, 3)
)
param[1]
# Compute means using formulas
means_formula <- sapply(params, function(param) compute_mean_formula(param[1], param[2]))
params[1]
params[2]
means_formula
# Simulate samples and compute means
means_simulation <- sapply(params, function(param) compute_mean_simulation(param[1], param[2]))
means_simulation
results <- data.frame(Method = c(rep("Formula", length(means_formula)), rep("Simulation", length(means_simulation))),
Distribution = rep(names(params), times = 2),
Mean = c(means_formula, means_simulation))
results
# Compare results
print(results)
# Check if means are approximately equal
all(approx(results$Mean[results$Method == "Formula"]) == results$Mean[results$Method == "Simulation"])
# Set the number of samples
n_samples <- 10000
# Simulate samples from Beta(1,1)
theta_samples <- rbeta(n_samples, 1, 1)
theta_samples
# Generate histogram of sampled values
hist(theta_samples, breaks = 30, main = "Histogram of Samples from Beta(1,1)", xlab = "Theta", ylab = "Frequency")
# Transform samples to odds and log(odds) scales
odds_samples <- theta_samples / (1 - theta_samples)
log_odds_samples <- log(odds_samples)
# Generate histogram of samples on the odds scale
hist(odds_samples, breaks = 30, main = "Histogram of Samples on the Odds Scale", xlab = "Odds", ylab = "Frequency")
# Generate histogram of samples on the log(odds) scale
hist(log_odds_samples, breaks = 30, main = "Histogram of Samples on the Log(Odds) Scale", xlab = "Log(Odds)", ylab = "Frequency")
prior <- rnorm(mean = 8, sd = 2)
hist(prior)
prior <- rnorm(mean = 8, sd = 2, n = 100)
hist(prior)
# prior normal density
# Define mean and standard deviation
mu <- 8
sigma <- 2
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using normal distribution function
density <- dnorm(x, mean = mu, sd = sigma)
# Plot the prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Prior Normal Density for Leaf Length")
sigma <- 1
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using normal distribution function
density <- dnorm(x, mean = mu, sd = sigma)
# Plot the prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Prior Normal Density for Leaf Length")
x
### 3.2 ###
# Define parameters for skewed normal distribution
mu <- 8
sigma <- 1
skewness <- 2  # Adjust this parameter to control the skewness
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using skewed normal distribution function
density <- dsn(x, mean = mu, sd = sigma, xi = skewness)
# Plot the updated prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Updated Prior Skewed Normal Density for Leaf Length")
### 3.2 ###
# Define parameters for skewed normal distribution
mu <- 8
sigma <- 1
skewness <- 2  # Adjust this parameter to control the skewness
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using skewed normal distribution function
density <- dsn(x, mean = mu, sd = sigma, xi = skewness)
# Plot the updated prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Updated Prior Skewed Normal Density for Leaf Length")
?dsn
dsn?
c
?dsn()
?dnorm
### 3.2 ###
# Install and load the 'sn' package
install.packages("sn")
library(sn)
# Define parameters for skewed normal distribution
mu <- 8
sigma <- 1
skewness <- 2  # Adjust this parameter to control the skewness
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using skewed normal distribution function
density <- dsn(x, mean = mu, sd = sigma, xi = skewness)
library(sn)
# Define parameters for skewed normal distribution
mu <- 8
sigma <- 1
skewness <- 2  # Adjust this parameter to control the skewness
# Generate sequence of x values
x <- seq(2, 14, length.out = 1000)
# Calculate density using skewed normal distribution function
density <- dsn(x, mean = mu, sd = sigma, xi = skewness)
?dsn
# Calculate density using skewed normal distribution function
density <- dsn(x, omega = mu, alpha = sigma, xi = skewness)
# Plot the updated prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Updated Prior Skewed Normal Density for Leaf Length")
# Calculate density using skewed normal distribution function
density <- dsn(x, omega = sd, alpha = mu, xi = skewness)
# Plot the updated prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Updated Prior Skewed Normal Density for Leaf Length")
# Calculate density using skewed normal distribution function
density <- dsn(x, omega = sd, alpha = mu, xi = skewness)
# Plot the updated prior density
plot(x, density, type = "l", lwd = 2, col = "blue",
xlab = "Leaf Length (cm)", ylab = "Density",
main = "Updated Prior Skewed Normal Density for Leaf Length")
data <- read.csv('data_alexithymia2.csv', sep = ';')
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/07')
data <- read.csv('data_alexithymia2.csv', sep = ';')
View(data)
str(data)
# remove variables: ID, sex
College2 = data[,-c(1,2, 24)]
# remove variables: ID, sex
data = data[,-c(1,2, 24)]
View(data)
data <- read.csv('data_alexithymia2.csv', sep = ';')
#exclude ID, sex, age, and CESD
TAS_20 = data[,-c(1,2,3, 24)]
# Define response and predictor variables
x = model.matrix(Apps~., data=data
)
data <- read.csv('data_alexithymia2.csv', sep = ';')
#exclude ID, sex, age
TAS_20 = data[,-c(1,2,3)]
# Define response and predictor variables
x = model.matrix(CESD~., data=data)
# Define response and predictor variables
x = model.matrix(CESD~., data=TAS_20)
y = TAS_20$CESD
train = sample( 1:nrow(x) , nrow(x)/2)
test = (-train)
x.test = x[test,]
y.test = y[test]
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
library(EFA.dimensions)
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
library(nFactors)
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
library(GPArotation)
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
library(pls)
library("pls")
install.packages("pls")
library("pls")
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
validationplot(pcr_fit_train, val.type="MSEP")
pcr_predict = predict(pcr_fit_train, x.test, ncomp=16)
pcr_predict = predict(pcr_fit_train, x.test, ncomp=20)
pcr_predict = predict(pcr_fit_train, x.test, ncomp=21)
x
data <- read.csv('data_alexithymia2.csv', sep = ';')
# Clear the entire workspace
rm(list = ls())
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/07')
data <- read.csv('data_alexithymia2.csv', sep = ';')
View(data)
#exclude ID, sex, age
TAS_20 = data[,-c(1,2,3)]
View(TAS_20)
# Define response and predictor variables
x = model.matrix(CESD~., data=TAS_20)
y = TAS_20$CESD
y
x
# Create trainng and test sets
set.seed(789)
train = sample( 1:nrow(x) , nrow(x)/2)
test = (-train)
x.test = x[test,]
y.test = y[test]
train
test
test
x.test
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
validationplot(pcr_fit_train, val.type="MSEP")
pcr_predict = predict(pcr_fit_train, x.test, ncomp=20)
pcr_predict = predict(pcr_fit_train, x.test, ncomp=19)
ncol(x.test)
pcr_predict = predict(pcr_fit_train, x.test, ncomp=21)
pcr_mse = mean((pcr_predict-y.test)ˆ2)
x.test
# Define response and predictor variables
x = model.matrix(CESD~., data=TAS_20)[,-1]
x
y = TAS_20$CESD
# Create trainng and test sets
set.seed(789)
train = sample( 1:nrow(x) , nrow(x)/2) #indices
test = (-train) #indices
x.test = x[test,] #data, takes indeces from design matrix
y.test = y[test]
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
validationplot(pcr_fit_train, val.type="MSEP")
pcr_predict = predict(pcr_fit_train, x.test, ncomp=20)
pcr_mse = mean((pcr_predict-y.test)ˆ2)
pcr_mse = mean((pcr_predict-y.test)^2)
pcr_mse
#fit PCR with 16 components to full data
pcr_fit_full = pcr(y~x, scale=TRUE, ncomp=20)
summary(pcr_fit_full)
# linear model coefficients
coef(pcr_fit_full)
# coefficients
coef(pcr_fit_full)
summary(pcr_fit_full)
pcr_fit_full$loadings
pcr_fit_full$Yloadings
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
validationplot(pcr_fit_train, val.type="MSEP")
#perform 10-fold cross-validation (on training data) to determine the number of components
#and estimate the test error (on validation data)
set.seed(20230315)
pcr_fit_train = pcr(CESD~., data=TAS_20, subset=train, scale=TRUE, validation="CV")
summary(pcr_fit_train)
validationplot(pcr_fit_train, val.type="MSEP")
validationplot(pcr_fit_train, val.type="MSEP")
pcr_predict = predict(pcr_fit_train, x.test, ncomp=20)
pcr_mse = mean((pcr_predict-y.test)^2) # 115.7976
pcr_mse
#fit PCR with 16 components to full data
pcr_fit_full = pcr(y~x, scale=TRUE, ncomp=13)
summary(pcr_fit_full)
# coefficients
coef
# coefficients
coef(pcr_fit_full)
pcr_fit_full$loadings
pcr_fit_full$Yloadings
# run PCA with normalized variables, mean 0 and unit variance
pca_result_sc = prcomp(TAS_20, scale=TRUE)
summary(pca_result_sc)
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
#variance
pc.var <- pca_result_sc$sdev^2
# proportion of variance explained by each principal component
pve <- pc.var / sum(pc.var)
pve
# plot variances explained by PC's
par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")
# Decicde on how many components to keep using Kaiser's rule and PVE
ev <- eigen(cor(CESD))
# Decicde on how many components to keep using Kaiser's rule and PVE
ev <- eigen(cor(TAS_20))
ev$values # we keep 3 PC's
summary(pcr_fit_train)
# Get predictior error for fitted PCA model
pcr_predict = predict(pcr_fit_train, x.test, ncomp=13)
pcr_mse = mean((pcr_predict-y.test)^2) # 115.7976
pcr_mse
pcr_mse
#fit PCR with 16 components to full data
pcr_fit_full = pcr(y~x, scale=TRUE, ncomp=13)
summary(pcr_fit_full)
# coefficients
coef(pcr_fit_full)
# coefficients
coef(pcr_fit_full)
View(pcr_fit_full)
pcr_fit_full$loadings
# relationship predictors to PC's
pcr_fit_full$loadings
# relationship response to PC's
pcr_fit_full$Yloadings
# relationship response to PC's
pcr_fit_full$Yloadings
pve
cumsum(pve)
validationplot(pcr_fit_train, val.type="MSEP") # MSEP -> lets keep 13 components, 91.67% variance explained on training data
# coefficients
coef(pcr_fit_full)
# relationship predictors to PC's
pcr_fit_full$loadings
# relationship response to PC's
pcr_fit_full$Yloadings
library(readr)
data_set <- read_csv("Data4036018.csv")
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/individual/08/')
setwd('/Users/joshd/Library/Mobile Documents/com~apple~CloudDocs/statistics_DS/statistical_learning/assignments/individual/01/')
data_set <- read_csv("Data4036018.csv")
rm()?
x
?rm()
rm(list = ls())
data_set <- read_csv("Data4036018.csv")
train <- data_set[1:5000, ]
test <- data_set[5001:10000, ]
View(data_set)
# Select relevant predictors (X1 to X3) and irrelevant predictors (X4 to X6)
predictors <- c("X1", "X2", "X3", "X4", "X5", "X6")
# Define training and test sets with relevant predictors
x_train <- train[, predictors]
y_train <- train$Y
x_test <- test[, predictors]
y_test <- test$Y
View(x_test)
# K-nearest neighbors (KNN) analysis
# Tune KNN model using cross-validation
knn_model <- train(x = x_train, y = y_train, method = "knn", trControl = trainControl(method = "cv"))
library(caret)
library(glmnet)
install.packages('caret')
# K-nearest neighbors (KNN) analysis
# Tune KNN model using cross-validation
knn_model <- train(x = x_train, y = y_train, method = "knn", trControl = trainControl(method = "cv"))
library(caret)
# K-nearest neighbors (KNN) analysis
# Tune KNN model using cross-validation
knn_model <- train(x = x_train, y = y_train, method = "knn", trControl = trainControl(method = "cv"))
install.packages('ModelMetrics')
library(ModelMetrics)
c
# K-nearest neighbors (KNN) analysis
# Tune KNN model using cross-validation
knn_model <- train(x = x_train, y = y_train, method = "knn", trControl = trainControl(method = "cv"))
library(readr)
library(caret)
library(glmnet)
library(ModelMetrics)
# K-nearest neighbors (KNN) analysis
# Tune KNN model using cross-validation
knn_model <- train(x = x_train, y = y_train, method = "knn", trControl = trainControl(method = "cv"))
# Make predictions
knn_pred <- predict(knn_model, newdata = x_test)
# Calculate misclassification error
knn_error <- sum(knn_pred != y_test) / length(y_test)
# LASSO logistic regression
# Fit LASSO logistic regression model
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, family = "binomial")
# Mean and mode values
mean_value <- 0.5
mode_value <- 0.8
beta_equation <- function(alpha) {
beta <- ((2 - alpha) * (mode_value - 1)) / (0.8 - mode_value)
return(beta)
}
# Solving for alpha using uniroot
alpha <- uniroot(beta_equation, c(0, 10))$root
beta <- beta_equation(alpha)
# Display alpha and beta
alpha
beta
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 0.833
beta <- 1.667
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 2
beta <- 2
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
rm?
c
?rm
rm(list = ls())
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 2
beta <- 2
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 2.5
beta <- 2.5
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
beta <- 4
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 4
beta <- 4
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 2.083
beta <- 2.083
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
# Using the calculated alpha and beta values to define the Beta distribution
alpha <- 4
beta <- 4
# Define the Beta distribution
prior_distribution <- rbeta(10000, alpha, beta)
# Plot the Beta distribution
hist(prior_distribution, breaks = 30, col = "skyblue", border = "white", main = "Prior Distribution (Beta)",
xlab = "Survival Proportion", ylab = "Density")
qbeta(0.025, 5, 4)
c(qbeta(0.025, alpha, beta), qbeta(0.975, alpha, beta))
