knitr::opts_chunk$set(echo = TRUE)
load("exercise2_grp28.RData")
# Function to perform Benjamini-Hochberg procedure
benjamini_hochberg <- function(p_values, alpha = 0.05) {
# Step 1: Sort the p-values
m <- length(p_values)
p_sorted <- sort(p_values)
original_order <- order(p_values)
# Step 2: Calculate the critical values and adjusted p-values
critical_values <- (1:m) / m * alpha
adjusted_p_values <- p.adjust(p_sorted, method = "BH")
# Step 3: Identify significant p-values
is_significant <- p_sorted <= critical_values
max_significant_index <- max(which(is_significant))
# Output results
results <- data.frame(
Rank = 1:m,
Original_p = p_values[original_order],
Sorted_p = p_sorted,
Critical_Value = critical_values,
Adjusted_p = adjusted_p_values,
Significant = p_sorted <= critical_values[max_significant_index]
)
# Restore original order
results <- results[order(original_order), ]
rownames(results) <- NULL
return(results)
}
# Run the BH procedure and display the results
results <- benjamini_hochberg(pall, alpha)
# Function to perform Benjamini-Hochberg procedure
benjamini_hochberg <- function(p_values, alpha = 0.05) {
# Step 1: Sort the p-values
m <- length(p_values)
p_sorted <- sort(p_values)
original_order <- order(p_values)
# Step 2: Calculate the critical values and adjusted p-values
critical_values <- (1:m) / m * alpha
adjusted_p_values <- p.adjust(p_sorted, method = "BH")
# Step 3: Identify significant p-values
is_significant <- p_sorted <= critical_values
max_significant_index <- max(which(is_significant))
# Output results
results <- data.frame(
Rank = 1:m,
Original_p = p_values[original_order],
Sorted_p = p_sorted,
Critical_Value = critical_values,
Adjusted_p = adjusted_p_values,
Significant = p_sorted <= critical_values[max_significant_index]
)
# Restore original order
results <- results[order(original_order), ]
rownames(results) <- NULL
return(results)
}
# Run the BH procedure and display the results
results <- benjamini_hochberg(pall, alpha = 0.05)
print(results
# Function to perform Benjamini-Hochberg procedure
benjamini_hochberg <- function(p_values, alpha = 0.05) {
# Step 1: Sort the p-values
m <- length(p_values)
p_sorted <- sort(p_values)
original_order <- order(p_values)
# Step 2: Calculate the critical values and adjusted p-values
critical_values <- (1:m) / m * alpha
adjusted_p_values <- p.adjust(p_sorted, method = "BH")
# Step 3: Identify significant p-values
is_significant <- p_sorted <= critical_values
max_significant_index <- max(which(is_significant))
# Output results
results <- data.frame(
Rank = 1:m,
Original_p = p_values[original_order],
Sorted_p = p_sorted,
Critical_Value = critical_values,
Adjusted_p = adjusted_p_values,
Significant = p_sorted <= critical_values[max_significant_index]
)
# Restore original order
results <- results[order(original_order), ]
rownames(results) <- NULL
return(results)
}
# Run the BH procedure and display the results
results <- benjamini_hochberg(pall, alpha = 0.05)
print(results)
# sort pvals
pval_sorted = sort(pall)
# total number of tests
m = length(pval_sorted)
# desired false discovery rate
FDR = q = 0.05
# Initialize vectors
threshold <- numeric(m)
BH_adjust <- numeric(m)
decision <- logical(m)
# Loop to calculate thresholds and adjust p-values
for (i in 1:m) {
threshold[i] <- i / m * q
}
for (i in 1:m) {
# Avoid referencing threshold[i + 1] when i == m
if (i < m) {
BH_adjust[i] <- pmin(threshold[i], threshold[i + 1])
} else {
BH_adjust[i] <- threshold[i]  # For the last element, just use threshold[i]
}
# Make decision based on comparison
if (pval_sorted[i] <= BH_adjust[i]) {
decision[i] <- TRUE
}
}
answer1 = BH_adjust[3]
answer1
p.adjust(pall, method = 'hochberg', n = length(pall))
?p.adjust
p.adjust(pall, method = 'BH', n = length(pall))
pvalues<- pall
ranks<-rank(pvalues, ties.method = "last")
p_m_over_k<-pvalues*length(pvalues)/ranks
for (r in length(pvalues):1) {
print(p_m_over_k[ranks>=r])
}
pvalues_adj<-c()
for (i in 1:length(pvalues)) {
# find the rank
tmp_rank<-ranks[i]
# get all the p_m_over_k that are greater or equal to this rank
# and get the min value
pvalues_adj<-c(pvalues_adj, min(1,min(p_m_over_k[ranks>=tmp_rank])))
}
print(pvalues_adj)
p.adjust(pall, method = "BH", n = length(pall))
p_adjusted = p.adjust(pall, method = "BH", n = length(pall))
p_adjusted = p.adjust(pall, method = "BH", n = length(pall))
sorted(p_adjusted)
p_adjusted = p.adjust(pall, method = "BH", n = length(pall))
sort(p_adjusted)
# sort pvals
pval_sorted = sort(pall)
# total number of tests
m = length(pval_sorted)
# desired false discovery rate
FDR = q = 0.05
# Initialize vectors
crit_val <- numeric(m)
BH_adjust <- numeric(m)
decision <- logical(m)
# Loop to calculate thresholds
for (i in 1:m) {
crit_val[i] <- i / m * q
}
for (i in 1:m) {
# Make decision based on comparison
if (pval_sorted[i] <= crti_val[i]) {
decision[i] <- TRUE
}
else {
decision[i] <- F
}
# adjust p-values
BH_adjust = pmin((m/i) * pval_sorted[i] , 1)
}
# sort pvals
pval_sorted = sort(pall)
# total number of tests
m = length(pval_sorted)
# desired false discovery rate
FDR = q = 0.05
# Initialize vectors
crit_val <- numeric(m)
BH_adjust <- numeric(m)
decision <- logical(m)
# Loop to calculate thresholds
for (i in 1:m) {
crit_val[i] <- i / m * q
}
for (i in 1:m) {
# Make decision based on comparison
if (pval_sorted[i] <= crit_val[i]) {
decision[i] <- TRUE
}
else {
decision[i] <- F
}
# adjust p-values
BH_adjust = pmin((m/i) * pval_sorted[i] , 1)
}
answer1 = BH_adjust[3]
answer1
BH_adjust
# sort pvals
pval_sorted = sort(pall)
# total number of tests
m = length(pval_sorted)
# desired false discovery rate
FDR = q = 0.05
# Initialize vectors
crit_val <- numeric(m)
BH_adjust <- numeric(m)
decision <- logical(m)
# Loop to calculate thresholds
for (i in 1:m) {
crit_val[i] <- i / m * q
}
for (i in 1:m) {
# Make decision based on comparison
if (pval_sorted[i] <= crit_val[i]) {
decision[i] <- TRUE
}
else {
decision[i] <- F
}
# adjust p-values
BH_adjust[i] = pmin((m/i) * pval_sorted[i] , 1)
}
answer1 = BH_adjust[3]
answer1
decision
crit_val
sorted_pvals
pvals_sorted
pval_sorted
BH_adjusted
BH_adjust
# Estimate the density of all raw p-values
p_density <- density(pall, from = 0, to = 1)
# plot mixture distribution
plot(p_density$x, p_density$y)
# Calculate f(1) and estimate pi_0
f_1 <- approx(p_density$x, p_density$y, xout = 1)$y  # Density at p = 1
pi_0 <- f_1  # Estimate of pi_0
# Calculate f(t) at t = 0.03
f_t <- approx(p_density$x, p_density$y, xout = 0.03)$y  # Density at t = 0.03
# Step 4: Calculate lFDR at t = 0.03, since f0(0.03) = 1
lfdr_t <- pi_0 / f_t
# Display result
answer2 = lfdr_t
# Estimate the density of all raw p-values
p_density <- density(pall, from = 0, to = 1)
# plot mixture distribution
plot(p_density$x, p_density$y)
# Calculate f(1) and estimate pi_0
f_1 <- approx(p_density$x, p_density$y, xout = 1)$y  # Density at p = 1
pi_0 <- f_1  # Estimate of pi_0
# Calculate f(t) at t = 0.03
f_t <- approx(p_density$x, p_density$y, xout = 0.03)$y  # Density at t = 0.03
# Step 4: Calculate lFDR at t = 0.03, since f0(0.03) = 1
lfdr_t <- pi_0 / f_t
# Display result
answer2 = lfdr_t
answer2
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
F_0
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
sum(p_pooled <= t)
pi_0 * m * F_0(t)
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi_0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
pi_0
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi_0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
pmat
pmat[,1]
p_pooled
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi_0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
# plot mixture distribution
plot(F_0$x, F_0$y)
F_0
F_0$x
F_0[,1]
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi_0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
#Plot the CDF
plot(F_0,
main = "Empirical CDF",
xlab = "Data values",
ylab = "Cumulative Probability",
col = "blue", # Optional: Set color for the line
lwd = 2)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
# get pooled p-value vector by pooling permutations, since i.i.d
p_pooled = c(pmat)
t = 0.01
m = ncol(pmat)
pi_0 = 1
# get cdfs
# cdf pooled, H0
F_0 = ecdf(p_pooled)
#Plot the CDF
plot(F_0,
main = "Empirical CDF",
xlab = "pvals",
ylab = "Cumulative Probability",
col = "blue", # Optional: Set color for the line
lwd = 2)
FDP_est = pi_0 * m * F_0(t) / (sum(p_pooled <= t))
answer3 = FDP_est
answer3
knitr::opts_chunk$set(echo = TRUE)
irish_polls = read.csv("irish_polls.csv")
df_clean = irish_polls[, 10:21]
# Replace Na
# iterate over all columns
for (col_idx in 1:ncol(df_clean)) {
# iterate over all rows
for (row_idx in 1:nrow(df_clean)) {
if (df_clean[row_idx, col_idx] == 'Not Available') {
# replace
df_clean[row_idx, col_idx] = NA
}
}
}
irish_polls = read.csv("irish_polls.csv")
df_clean = irish_polls[, 10:21]
for (i in 10:21) {
na.pos = which(irish_polls[ , i] == 'Not Available')
if (length(na.pos) > 0) irish_polls[na.pos, i] = NA
irish_polls[ , i] = substr(irish_polls[ , i], 1, nchar(irish_polls[ , i]) - 1)
irish_polls[ , i] = as.numeric(irish_polls[ , i])
}
# select first 10 polls
df_first = irish_polls[, 1:10]
# select first 10 polls
df_first = irish_polls[1:10,]
View(irish_polls)
# select first 10 polls
df_first = irish_polls[1:10,]
df_first$party_means = NA
View(df_first)
df_first$party_means[1]
df_first$party_means[2]
View(irish_polls)
View(df_first)
# select first 10 polls
df_first = irish_polls[1:10,]
colnames[df_first[,10:21]]
colnames(df_first[,10:21])
?sapply
# select first 10 polls
df_first = irish_polls[1:10,]
df_parties = data.frame(
party = colnames(df_first[,10:21]),
mean_vote = sapply(df_first[, 10:21], mean, na.rm = T)
)
df_parties
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first[, 10:21], mean, na.rm = T)
)
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
?which
View(df_parties)
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
which(df_parties$mean_vote > 6)
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
df_parties$party[which(df_parties$mean_vote > 6)]
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
df_parties$mean_vote[which(df_parties$mean_vote > 6)]
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
df_parties$party[which(df_parties$mean_vote > 6)]
# select first 10 polls
df_first = irish_polls[1:10, 10:21]
df_parties = data.frame(
party = colnames(df_first),
mean_vote = sapply(df_first, mean, na.rm = T)
)
# mean share above 6%?
df_parties$party[which(df_parties$mean_vote > 6)]
df_parties$mean_vote[which(df_parties$mean_vote > 6)]
?barplot
barplot(df_first)
barplot(heights = 10, data = df_first)
barplot(height = 10, data = df_first)
barplot(df_parties$mean_vote)
barplot(sort(df_parties$mean_vote)
barplot(sort(df_parties$mean_vote))
?sort
# sorted mean votes (descending)
shares_sorted = sort(df_parties$mean_vote, decreasing = T)
barplot(shares_sorted)
?barplot
# sorted mean votes (descending)
shares_sorted = sort(df_parties$mean_vote, decreasing = T)
barplot(shares_sorted, xlab = colnames(df_first))
colnames(df_first)
# sorted mean votes (descending)
shares_sorted = sort(df_parties$mean_vote, decreasing = T)
barplot(shares_sorted, names.arg = colnames(df_first))
# sorted mean votes (descending)
shares_sorted = sort(df_parties$mean_vote, decreasing = T)
barplot(shares_sorted, names.arg = df_parties$party)
df_parties$party
# sorted mean votes (descending)
shares_sorted = sort(df_parties$mean_vote, decreasing = T)
barplot(shares_sorted, names.arg = df_parties$party[-2])
